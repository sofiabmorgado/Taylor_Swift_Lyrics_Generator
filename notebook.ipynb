{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taylor Swift's Lyrics Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the dataset of Taylor Swift songs, write a song generator that will continue writing a song given the first 2-3 verses in the style of the mentioned artist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re #https://docs.python.org/3/library/re.html\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#Evaluation of the results\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data \n",
    "directory = 'data/Albums/'\n",
    "\n",
    "lyrics_list = [] #list with all the lyrics\n",
    "\n",
    "for album_name in os.listdir(directory):\n",
    "    album_path = os.path.join(directory, album_name)\n",
    "    for music_name in os.listdir(album_path):\n",
    "        music_path = os.path.join(album_path, music_name)\n",
    "        with open(music_path, 'r', encoding='utf-8') as file:\n",
    "            music_lyrics = file.read()\n",
    "            lyrics_list.append(music_lyrics)\n",
    "\n",
    "#Create a dataframe with all the lyrics from all the albums from Taylor Swift\n",
    "lyrics_df = pd.DataFrame({'Lyrics': lyrics_list})\n",
    "\n",
    "print(\"Number of lyrics: \", lyrics_df.shape[0]) \n",
    "print(lyrics_df) #print examples of lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO remove this. this was used due to time constraints but must be removed for complete training\n",
    "#Shuffle and choose some for quicker training\n",
    "#lyrics_df = lyrics_df.sample(frac = 1) #shuffle the songs\n",
    "#lyrics_df = lyrics_df.head(200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show one example\n",
    "print(lyrics_df[\"Lyrics\"].iloc[0])\n",
    "\n",
    "#We should remove the first sentence \"104 ContributorsTranslationsEspañolPortuguêsFrançaisClean\" that does not belong to the music\n",
    "#We should remove the last sentence \"161Embed\" #See how it varies for different examples\n",
    "#We should remove things that are between [] as they do not belong to the lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is a prefix in every lyrics that must be removed\n",
    "def remove_prefix(lyrics):\n",
    "    \"\"\"Remove the prefix the text contains before the beggining of the lyrics\"\"\"\n",
    "    prefix_position = lyrics.find(\"Lyrics\") #the position of the first word in Lyrics\n",
    "    return lyrics[prefix_position + 6:] #Remove eeverything before, and including, \"Lyrics\"\n",
    "\n",
    "#Remove labels\n",
    "def remove_reg_expressions(lyrics):\n",
    "    \"\"\"Remove labels such as [Verse 1], [Guitar], etc and remove last sentence emb/kemb\"\"\"\n",
    "    lyrics = re.sub(r'\\[.*?\\]', '', lyrics)\n",
    "    lyrics = re.sub('[0-9]+KEmbed', '', lyrics)\n",
    "    lyrics = re.sub('[0-9]+Embed', '', lyrics)\n",
    "    return lyrics\n",
    "\n",
    "\n",
    "lyrics_df['Lyrics'] = lyrics_df['Lyrics'].apply(remove_prefix)\n",
    "lyrics_df['Lyrics'] = lyrics_df['Lyrics'].apply(remove_reg_expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lyrics_df['Lyrics'].iloc[1]) #Show one clean example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing values \n",
    "lyrics_df.isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicates\n",
    "print(f\"Number of duplicates: {lyrics_df['Lyrics'].duplicated().sum()}\") #How many duplicates were there? \n",
    "\n",
    "#Remove duplicates\n",
    "lyrics_df = lyrics_df.drop_duplicates(subset='Lyrics', keep='first') #Remove duplicates\n",
    "print(f\"Number of lyrics after duplicates removal: {lyrics_df.shape[0]}\") #Size after duplicates removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle and split into training and test set\n",
    "lyrics_df = lyrics_df.sample(frac = 1) #shuffle the songs\n",
    "df_train, df_test = train_test_split(lyrics_df, test_size=0.01) #A very small amount of lyrics will be separated for \"test\": the first 2/3 verses will be used as a prompt to the lyrics generator; This step may be removed if we want to create prompts by hand.\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.1) #Define a train and validation sets for parameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the test set, create a column with the first 3 verses of the lyrics that will be used as a initializing prompt for the word generator\n",
    "def select_first_3_verses(lyrics):\n",
    "    \"\"\"Select the first 3 verses from the lyrics.\"\"\"\n",
    "    lines = lyrics.split(\"\\n\")\n",
    "    first_3_verses = \"\\n\".join(lines[:4]) \n",
    "    return first_3_verses\n",
    "\n",
    "\n",
    "df_test['First_3_verses'] = df_test['Lyrics'].apply(select_first_3_verses)\n",
    "print(df_test[\"First_3_verses\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    #CustomImageDataset extends Dataset from https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "    #It must implement three functions, __init__, __len__ and __getitem__\n",
    "\n",
    "    def __init__(self, all_lyrics, tokenizer):\n",
    "        \"\"\"Using the data_set/list all_lyrics and the tokenizer, we get two torch tensors, \n",
    "         the input_ids and the attention_mask\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.input_ids = []\n",
    "        self.attention_mask = []\n",
    "\n",
    "        for lyrics in all_lyrics:\n",
    "            encoding = tokenizer('<|startoftext|>'+ lyrics + '<|endoftext|>',  #'<|startoftext|>'+ \n",
    "                                truncation=True, \n",
    "                                max_length=tokenizer.model_max_length, #1024\n",
    "                                padding=\"max_length\",\n",
    "                                ) #pad until it reaches maximum lenght\n",
    "\n",
    "            #encoding['attention_mask'][0] = 0 \n",
    "            self.input_ids.append(torch.tensor(encoding['input_ids']))\n",
    "            self.attention_mask.append(torch.tensor(encoding['attention_mask'])) #will be 0 for the padded elements\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"The __len__ function returns the number of samples in our dataset.\"\"\"\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"The __getitem__ function loads and returns a sample from the dataset at the given index index.\"\"\"\n",
    "        image = self.input_ids[index]\n",
    "        label = self.attention_mask[index] \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the tokenizer to be used. Define the pad_token as for GPT there is no default\n",
    "model_name = \"gpt2\" \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name) #, bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "tokenizer.pad_token = tokenizer.eos_token #using a token of end of text to pad\n",
    "#tokenizer.pad_token ='[PAD]'\n",
    "\n",
    "#Get the dataset using CustomImageDataset\n",
    "dataset_train = CustomImageDataset(df_train[\"Lyrics\"], tokenizer)\n",
    "dataset_val = CustomImageDataset(df_val[\"Lyrics\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloader: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "#While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting\n",
    "#DataLoader is an iterable that abstracts this complexity\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True,\n",
    "                              pin_memory=True) #pin-memory \n",
    "\n",
    "val_dataloader = DataLoader(dataset_train, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=True,\n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choosing the specific model**\n",
    "\n",
    "GPT2Model is the base GPT-2 model and it is usefull for several tasks, including feature extraction, but is not indicated for text generation. \n",
    "\n",
    "**GPT2LMHeadModel** is specifically designed for language modeling and text generation tasks. It has a LM Head, or Language Model Head, which is a component added to a pre-trained model to adapt it for language generation tasks. It’s essentially a linear layer that predicts the next token in a sequence, given the previous tokens.\n",
    "\n",
    "GPT2DoubleHeadsModel is a more flexible model that can handle both language modeling and other tasks, making it suitable for multitask learning scenarios. It is a more complex model due to its dual heads, harder to work with and can consume more computational resources.\n",
    "\n",
    "GPT2LMHeadModel is primarily built for PyTorch, while TFGPT2LMHeadModel is also based on GPT-2 but is intended for TensorFlow. We will use GPT2LMHeadModel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose if we want to do something in the configuration and create model\n",
    "#configuration = GPT2Config.from_pretrained('gpt2', vocab_size = 50257, n_positions = 1024) #using the default. Change the params here. \n",
    "model = GPT2LMHeadModel.from_pretrained(model_name) #, pad_token_id=tokenizer.eos_token_id) #config=configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the best optimizer\n",
    "#https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "#Define the parameters\n",
    "learning_rate = 1e-3 \n",
    "epsilon = 1e-8 #term added to the denominator to improve numerical stability \n",
    "weight_decay = 1e-2 #weight decay coefficient = L2 regularization applied. Adding a penalty term to the loss function during training that discourages large values for the model's parameters (weights). The regularization term is defined as the sum of squared values of all model parameters. \n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, eps = epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "train_loss_per_epoch = [] #Initialize the train loss list per epoch \n",
    "val_loss_per_epoch = [] #Initialize the validation loss list per epoch \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(epoch)\n",
    "    \n",
    "    ## Training ##\n",
    "    total_train_loss = 0 #Initialize the loss for this epoch as 0\n",
    "    model.train() #Set the model in training mode. In this mode, dropout and batch normalization layers introduce randomness during training to prevent overfitting.\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        #From each batch, take the input_ids and the attention_mask\n",
    "        input_ids = batch[0]\n",
    "        attention_mask = batch[1]\n",
    "\n",
    "        #Clean the information from previous batches \n",
    "        model.zero_grad()  #Clear any previously calculated gradients before performing a backward pass\n",
    "        total_loss = 0 #Set loss to 0\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(input_ids,\n",
    "                        labels=input_ids, \n",
    "                        attention_mask = attention_mask)   \n",
    "\n",
    "        #Get the loss values\n",
    "        loss = outputs.loss #Loss for this bacht\n",
    "        total_train_loss += loss.item() #Get the value with item() and add the loss of this batch to the list of losses \n",
    "\n",
    "    #Get the average loss for this epoch: Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)    \n",
    "    train_loss_per_epoch.append(avg_train_loss)\n",
    "\n",
    "    loss.backward() #Backpropagation: Perform a backward pass to calculate the gradients. \n",
    "    optimizer.step() #Parameter updating based on the computed gradients, learning rate, optimizer\n",
    "\n",
    "\n",
    "\n",
    "    ## Validation ##\n",
    "    total_val_loss = 0\n",
    "    model.eval() #Set the model in evaluation mode. Dropout layers do not drop any units during evaluation and batch normalization layers use population statistics (averages and variances collected during training) instead of batch-specific statistics. This ensures consistent and deterministic behavior during inference.\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        #From each batch, take the input_ids and the attention_mask\n",
    "        input_ids = batch[0]\n",
    "        attention_mask = batch[1]\n",
    "\n",
    "        #Clean the information from previous batches \n",
    "        model.zero_grad()  #Clear any previously calculated gradients before performing a backward pass\n",
    "        total_loss = 0 #Set loss to 0\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(input_ids,\n",
    "                        labels=input_ids, \n",
    "                        attention_mask = attention_mask)     \n",
    "\n",
    "        #Calculate the loss for this batch\n",
    "        loss = outputs.loss\n",
    "        #Add the loss of this batch to the list of losses \n",
    "        total_val_loss += loss.item() #Get the value\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_loss_per_epoch.append(avg_train_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the loss function for each epoch\n",
    "results = {\"epoch_list_ks\": np.arange(1, epochs + 1, 1),\n",
    "           \"loss_function\": train_loss_per_epoch,\n",
    "           \"validation_loss_function\": val_loss_per_epoch}\n",
    "\n",
    "#We create a dataframe so we can apreciate the evolution of the loss function through the epochs\n",
    "df_loss = pd.DataFrame(data = results)[[\"loss_function\", \"validation_loss_function\"]]\n",
    "\n",
    "#Create a plot\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "sns.lineplot(data=df_loss)\n",
    "plt.title('Value of the loss function for each epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss function value')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/tokenizer_config.json',\n",
       " './model/special_tokens_map.json',\n",
       " './model/vocab.json',\n",
       " './model/merges.txt',\n",
       " './model/added_tokens.json')"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the model\n",
    "output_dir = './model/'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to use the model pre-trained, you can find the link in the README file.\n",
    "\n",
    "#Load the saved model\n",
    "model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Lyrics\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = CustomImageDataset(df_test[\"Lyrics\"], tokenizer)\n",
    "tensor = dataset_test[0][0]\n",
    "num_tokens = tensor.size(0) \n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You booked the night train for a reason\n",
      "So you could sit there in this hurt\n",
      "Bustling crowds or silent sleepers\n"
     ]
    }
   ],
   "source": [
    "#If we want to re-write some of TS songs based on their first 2 or 3 verses, use this:\n",
    "test_example = 0\n",
    "text_to_insert = df_test[\"First_3_verses\"].iloc[test_example]\n",
    "print(text_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  198,  1639, 21765,   262,  1755,  4512,   329,   257,  1738,   198,\n",
      "         2396,   345,   714,  1650,   612,   287,   428,  5938,   198,    33,\n",
      "          436,  1359, 15779,   393, 10574,  3993,   364,   318,   691,  2219,\n",
      "          706,   257,  6388,   319,   257,   922,  7374,  3931,   618,   345,\n",
      "          389,  1654,  2506, 14759,   986,   628,   366, 10995,   616,  6621,\n",
      "          508,  2925,   351,   502,   736,   284,  3240,   262,  1285,   357,\n",
      "          271, 10091,   481,  1282,   351,   674,  3656,   532,   674,   604,\n",
      "           11,   607,  1115, 11875,   287,  2166,  1222,   502,   530,   503,\n",
      "          492,     1, 11485,  1106,   986,   356,  1541,   423,   534, 13008,\n",
      "          492,     7,    82, 17403,   798, 23029,    40,   731,   534,   362,\n",
      "          812,  2084,   389,   655,  8523,  2877,   379,   604,   393,  2029,\n",
      "          314,   714,   307,   287,   767,  2250,    11,   616,  2802,   373,\n",
      "        11029,   362,    12,    16,    10,  1528,   981,   428,   318,   362,\n",
      "           10, 12545,   284,   838,    25,  1415,     7,  5832,  1265, 29865,\n",
      "         1649,   607,  1755,    11,   475,   428,  2187,   983,   986, 12518,\n",
      "          326,  1110,   287,   674,  2119,   326,   607,  1545,   286,   530,\n",
      "          508,  3111,  1327,   373,  4305,   319,   352,    14,   820,   345,\n",
      "          550,  1297,   883,   286,   511, 34015,   673,  2476,   345,   760,\n",
      "          607,   287,   262,  7541,   326,  3329,  1106,   262,  1807,   284,\n",
      "         1650,   625,  1222,   407,  1337,   318,  6639,   263,   379,   257,\n",
      "         7099,  2612,   621,  5586,   588,   262,   717,  1517,   503,   345,\n",
      "          290,   986,     7,   345,   651,   616,   474, 32438,  1267,   366,\n",
      "        50256])\n",
      "\n",
      "You booked the night train for a reason\n",
      "So you could sit there in this hurt\n",
      "Bustling crowds or silent sleepers is only common after a storm on a good winter summer when you are sure everyone understands...\n",
      "\n",
      " \"Yeah my sister who goes with me back to town the week (is?) will come with our wife - our 4, her three cats in front & me one out..\"......... we already have your wallet..(satisfied...)I bet your 2 years ago are just barely living at 4 or above I could be in 7 hours, my mother was sleeping 2-1+ days while this is 2+days to 10:14(you ask?). When her night, but this whole game...when that day in our room that her friend of one who worked hard was leaving on 1/day you had told those of their moms she needs you know her in the hotel that morning.... the thought to sit over & not care is sicker at a younger heart than sitting like the first thing out you and...( you get my jibe ) \"\n"
     ]
    }
   ],
   "source": [
    "model.eval() #model.eval() is used to ensure consistent and deterministic behavior of the model during text generation.\n",
    "\n",
    "#Create the prompt and tokenize it\n",
    "prompt = text_to_insert\n",
    "input_ids = torch.tensor(tokenizer.encode(prompt, add_special_tokens=True)).unsqueeze(0)\n",
    "\n",
    "#Generate text\n",
    "output = model.generate(input_ids, #starting point\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                        min_new_tokens = 150, #Here we can use a range that is common for TS songs\n",
    "                        max_new_tokens = 300,\n",
    "                        do_sample=True, #it enables random sampling of tokens during generation, adding some randomness to the result obtained\n",
    "                        temperature=10.0, #Controls the randomness of the generated text. A higher temperature makes the text more random by making a picked probability distribution of the words, creative, diverse, while a lower temperature makes it more deterministic and focused, flatter prob distribution\n",
    "                        #top_k=3, #The model considers the top-k most likely tokens at each step.\n",
    "                        #top_p=100, #cumulative probability >p in the ordered results\n",
    "                        num_return_sequences=1, #Returning only one text for now\n",
    "                        )\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output[0]) #Show the sequence of generated tokens\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the generated lyrics \n",
    "with open(\"generated_lyrics/generated_lyrics\", \"w\") as text_file:\n",
    "    text_file.write(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the first three verses that were given \n",
    "original_text = df_test[\"Lyrics\"].iloc[test_example]\n",
    "\n",
    "original_without_beginning = original_text[len(prompt):]\n",
    "generated_without_beggining = generated_text[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.42038216560509556, recall=0.22837370242214533, fmeasure=0.2959641255605381),\n",
       " 'rouge2': Score(precision=0.019230769230769232, recall=0.010416666666666666, fmeasure=0.013513513513513514),\n",
       " 'rougeL': Score(precision=0.16560509554140126, recall=0.08996539792387544, fmeasure=0.11659192825112107)}"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scores\n",
    "#Rouge for summarization\n",
    "#– summaries, compares to one or more reference summaries, -recall, -precision\n",
    "#-does not care about the order if using unigrams, for bigrams we care about the order of pairs of words #-Rouge-L – longest common sequence\n",
    "#GLUE scores are usually computed on natural language understanding tasks,\n",
    "\n",
    "\n",
    "#Rouge Score\n",
    "#https://huggingface.co/spaces/evaluate-metric/rouge\n",
    "#https://pypi.org/project/rouge-score/\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True) #rouge1 - n-gram scoring; rougeL - longest common seq\n",
    "rouge_score = scorer.score(original_without_beginning, \n",
    "                      generated_without_beggining) #compare generated text with the original lyrics\n",
    "rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sofia\\Ambiente de Trabalho\\Taylor_Swift_Lyrics_Generator\\venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\sofia\\Ambiente de Trabalho\\Taylor_Swift_Lyrics_Generator\\venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\sofia\\Ambiente de Trabalho\\Taylor_Swift_Lyrics_Generator\\venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.057672682463324e-232"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bleu is commonly used for machine translation evaluation but can be adapted for text generation.\n",
    "#Range from 0 to 1: higher scores indicating better quality and similarity \n",
    "\n",
    "# Calculate BLEU score for a single sentence\n",
    "bleu_score = sentence_bleu(original_without_beginning, generated_without_beggining)\n",
    "bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://huggingface.co/docs/transformers/model_doc/gpt2\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "\n",
    "https://colab.research.google.com/drive/13dZVYEOMhXhkXWfvSMVM1TTtUDrT6Aeh?usp=sharing#scrollTo=v4XhewaV93-_\n",
    "\n",
    "https://gmihaila.github.io/tutorial_notebooks/gpt2_finetune_classification/\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
